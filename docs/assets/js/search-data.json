{"0": {
    "doc": "Attributions",
    "title": "Attributions",
    "content": ". | https://github.com/lokesh-coder/pretty-checkbox | https://github.com/leongersen/noUiSlider | https://github.com/pure-css/pure/ | https://github.com/fabricjs/fabric.js | . ",
    "url": "/attributions/",
    "relUrl": "/attributions/"
  },"1": {
    "doc": "Bytes, Bits, and Nibbles",
    "title": "Bytes, Bits, and Nibbles",
    "content": "Stub. ",
    "url": "/reference/bytes/",
    "relUrl": "/reference/bytes/"
  },"2": {
    "doc": "Compression Ratios",
    "title": "Compression Ratios",
    "content": "Stub. https://en.wikipedia.org/wiki/Data_compression_ratio . ",
    "url": "/reference/compression_ratios/",
    "relUrl": "/reference/compression_ratios/"
  },"3": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": "This site is intended to be used as a living documentation that is constantly updated by the community, if you feel like something‚Äôs missing or would like to contribute, please do one of the following create an issue or create a pull request. The best way to ask a question is to file an issue and we‚Äôll try to get back to you. ",
    "url": "/contributing/",
    "relUrl": "/contributing/"
  },"4": {
    "doc": "File Encodings",
    "title": "File Encodings",
    "content": "Stub. ",
    "url": "/reference/encodings/",
    "relUrl": "/reference/encodings/"
  },"5": {
    "doc": "Fullscreen LZ77/LZSS",
    "title": "Fullscreen LZ77/LZSS",
    "content": "The fullscreen version: . | Does not resize content to fit your canvas | Allows you to pan and zoom | Has a larger canvas window | . ",
    "url": "/interactive/fullscreen_lz/",
    "relUrl": "/interactive/fullscreen_lz/"
  },"6": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "This article is currently a stub. ",
    "url": "/getting_started/",
    "relUrl": "/getting_started/"
  },"7": {
    "doc": "Huffman",
    "title": "Huffman",
    "content": "Since it‚Äôs creation by David A. Huffman in 1952, Huffman coding has been regarded as one of the most efficient and optimal methods of compression. Huffman‚Äôs optimal compression ratios are made possible through it‚Äôs character counting functionality. Unlike many algorithims in the Lempel-Ziv suite, Huffman encoders scan the file and generate a frequency table and tree before begining the true compression process. Before discussing different implementations, lets dive deeper into how the algorithim works. ",
    "url": "/algorithms/huffman/",
    "relUrl": "/algorithms/huffman/"
  },"8": {
    "doc": "Huffman",
    "title": "The Algorithm",
    "content": "Although huffman encoding may seem confusing from an outside view, we can break it into three simple steps: . | Frequency Counting | Tree Building | Character Encoding | . ",
    "url": "/algorithms/huffman/#the-algorithm",
    "relUrl": "/algorithms/huffman/#the-algorithm"
  },"9": {
    "doc": "Huffman",
    "title": "Frequency Countinig",
    "content": "Let‚Äôs start out by going over the frequency counting step. Throughout all of the examples, I will be using the following sample input string: . I AM SAM. I AM SAM. SAM I AM. THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM! . The huffman encoder starts out by going over the inputed text and outputing a table correlating each character to the number of time it appears in the text. For the sample input, the table would look this: . | Frequency | Character | . | 1 | N | . | 1 | \\n | . | 1 | K | . | 1 | D | . | 1 | L | . | 1 | E | . | 2 | O | . | 3 | H | . | 3 | ! | . | 3 | . | . | 6 | - | . | 6 | S | . | 7 | T | . | 8 | I | . | 12 | M | . | 15 | A | . As displayed above, the table is sorted to ensure consistency in each step of the compression process. ",
    "url": "/algorithms/huffman/#frequency-countinig",
    "relUrl": "/algorithms/huffman/#frequency-countinig"
  },"10": {
    "doc": "Overview",
    "title": "The Hitchhiker‚Äôs Guide to Compression",
    "content": "Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape- descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea. This planet has - or rather had - a problem, which was this: files were too big. Many solutions were suggested for solving this problem via lossless compression, such as Lempel-Ziv and Huffman coding, but most of these were implemented into common compression utilities and promptly forgotten. Today, much of the relevant work to compression is in an obscure corner of the internet between lengthy PhD thesis papers and hard-to-find gems. ",
    "url": "/#the-hitchhikers-guide-to-compression",
    "relUrl": "/#the-hitchhikers-guide-to-compression"
  },"11": {
    "doc": "Overview",
    "title": "Why compression",
    "content": "Lossless file compression, and file compression in general has become a lost art. The modern developer community has moved on from working on compression algorithms to bigger and better problems, such as creating the next major NodeJS framework. However, compression as it stands in the computer science aspect is still as interesting as it was in 1980s, possibly even more so today with an estimated 463 Exabytes of data to be created everyday in 2025. It‚Äôs no secret that the internet is growing rapidly, and with it more people are becoming connected. From urban dwellers to rural farmers, fast internet speed are not a given. To counter this, there are numerous projects focused on improving internet speeds for rural users, but there are almost no projects focused on the other half of improving internet access: compressing data. These claims about the ‚Äúlost of art of compression‚Äù may seem a bit unsubstantiated, as there are new and actively developed compression projects out there today, such as, but not limited to: . | Facebook‚Äôs ZSTD | Google‚Äôs Brotli | LZ4 | Shrynk | . However this argument still holds true, compression isn‚Äôt really mainstream, and I don‚Äôt know why it isn‚Äôt. Internet speeds is a real problem and better compression stands as a promising solution. The possibilities of better compression are truly endless: . | Faster 4k video streaming | Faster app downloads | Less delay loading websites and content | and more | . ",
    "url": "/#why-compression",
    "relUrl": "/#why-compression"
  },"12": {
    "doc": "Overview",
    "title": "The Goal",
    "content": "The goal of this project, and by extension, the goal of all resources here is to help people learn about compression algorithms and encourage people to tinker, build, and experiment with their own algorithms and implementations. Afterall, the best way to innovate in tech is to get a bunch of developers interested in something and let them lead the way. Additionally, this project itself is intended to be a community-sourced resource for people interested in compression algorithms. The idea is that anyone can contribute to this website through GitHub so that this can be a constantly improving and expanding resource for others. With all of that said, if you‚Äôre interested in learning more about the world of compression, you should get started. ",
    "url": "/#the-goal",
    "relUrl": "/#the-goal"
  },"13": {
    "doc": "Overview",
    "title": "Notable Compression Project Mentions",
    "content": "There are also some other notable projects which I‚Äôve included at the end, but either they aren‚Äôt active enough or univeral to be included here. Notable mentions: . | LZHAM (not super active) | Google‚Äôs WebP (only for images) | Dropbox‚Äôs DivANS | Dropbox‚Äôs avrecode | H.266/VCC Codec (domain-specific for video) | Pied Piper Middle-Out (abandoned and closed-source unfortunately) | . ",
    "url": "/#notable-compression-project-mentions",
    "relUrl": "/#notable-compression-project-mentions"
  },"14": {
    "doc": "Overview",
    "title": "Overview",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"15": {
    "doc": "Interactive Algorithms",
    "title": "Interactive Algorithms",
    "content": "Stub. ",
    "url": "/interactive/interactive/",
    "relUrl": "/interactive/interactive/"
  },"16": {
    "doc": "LZ77/LZSS",
    "title": "LZ77/LZSS",
    "content": " ",
    "url": "/interactive/lz/",
    "relUrl": "/interactive/lz/"
  },"17": {
    "doc": "Lempel-Ziv",
    "title": "Lempel-Ziv",
    "content": "Lempel-Ziv, commonly referred to as LZ77/LZ78 depending on the variant, is one of the oldest, most simplistic, and widespread compression algorithms out there. It‚Äôs power comes from its simplicity, speed, and decent compression rates. Now before we dive into an implementation, let‚Äôs understand the concept behind Lempel-Ziv and the various algorithms it has spawned. ",
    "url": "/algorithms/lz/",
    "relUrl": "/algorithms/lz/"
  },"18": {
    "doc": "Lempel-Ziv",
    "title": "The Algorithm(s)",
    "content": "Lempel-Ziv at its core is very simple. It works by taking an input string of characters, finding repetitive characters, and outputting an ‚Äúencoded‚Äù version. To get an idea of it, here‚Äôs an example. Original: Hello everyone! Hello world! Encoded: Hello everyone! &lt;16,6&gt;world! . As you can see, the algorithm simply takes an input string, in this case, ‚ÄúHello everyone! Hello world!‚Äù, and encodes it character by character. If it tries to encode a character it has already seen it will check to see if it has seen the next character. This repeats until it the character it‚Äôs checking hasn‚Äôt been seen before, following the characters it‚Äôs currently encoding, at this point it outputs a ‚Äútoken‚Äù, which is &lt;16,6&gt; in this example, and continues. The &lt;16,6&gt; token is quite simple to understand too, it consists of two numbers and some syntactic sugar to make it easy to understand. The first number corresponds to how many characters it shoud look backwards, and the next number tells it how many characters to go forwards and copy. This means that in our example, &lt;16,6&gt; expands into ‚ÄúHello ‚Äú as it goes 16 characters backwards, and copies the next 6 characters. This is the essential idea behind the algorithm, however it should be noted that there are many variations of this algorithm with different names. For example, in some implementations, the first number means go forwards from the beginning instead of backwards from the current position. Small (and big) differences like these are the reason for so many variations: . | LZSS - Lempel-Ziv-Storer-Szymanski | LZW - Lempel-Ziv-Welch | LZMA - Lempel‚ÄìZiv‚ÄìMarkov chain algorithm | LZ77 - Lempel-Ziv 77 | LZ78 - Lempel-Ziv 78 | . It‚Äôs also important to understand the difference between LZ77 and LZ78, the first, and most common, Lempel-Ziv algorithms. LZ77 works very similarly to the example above, using a token to represent an offset and length, while LZ78 uses a more complicated dictionary approach. For a more in-depth explanation, make sure to check out this wonderful article explaining LZ78. UNFINISHED . ",
    "url": "/algorithms/lz/#the-algorithms",
    "relUrl": "/algorithms/lz/#the-algorithms"
  },"19": {
    "doc": "Lempel-Ziv",
    "title": "Implementations",
    "content": "Now because there are so many different variations of Lempel-Ziv algorithms, there isn‚Äôt a single LZ implementation. WIth that being said, if you are interested in implementing a Lempel-Ziv algorithm yourself, you‚Äôll have to choose an algorithm to start with. LZSS is a great starting point as it‚Äôs a basic evolution of LZ77 and can be implemented very easily while achieving a respectable compression ratio. If you‚Äôre interested in another algorithm, head back to the algorithms . ",
    "url": "/algorithms/lz/#implementations",
    "relUrl": "/algorithms/lz/#implementations"
  },"20": {
    "doc": "LZSS",
    "title": "Lempel-Ziv-Storer-Szymanski",
    "content": "Lempel-Ziv-Storer-Szymanski, which we‚Äôll refer to as LZSS, is a simple variation of the common LZ77 algorithm. It uses the same token concept with an offset and length to tell the decoder where to copy the text, except it only places the token when the token is shorter than the text it is replacing. The idea behind this is that it will never increase the size of a file by adding tokens everywhere for repeated letters. You can imagine that LZ77 would easily increase the file size if it simply encoded every repeated letter ‚Äúe‚Äù or ‚Äúi‚Äù as a token, which may take at least 5 bytes depending on the file and implementation. ",
    "url": "/algorithms/lzss/#lempel-ziv-storer-szymanski",
    "relUrl": "/algorithms/lzss/#lempel-ziv-storer-szymanski"
  },"21": {
    "doc": "LZSS",
    "title": "Implementing an Encoder",
    "content": "Let‚Äôs take a look at some examples, so we can see exactly how it works. The wikipedia article for LZSS has a great example for this, which I‚Äôll use here, and it‚Äôs worth a read as an introduction to LZSS. So let‚Äôs encode an exceprt of Dr. Seuss‚Äôs Green Eggs and Ham with LZSS (credit to Wikipedia for this example). I AM SAM. I AM SAM. SAM I AM. THAT SAM-I-AM! THAT SAM-I-AM! I DO NOT LIKE THAT SAM-I-AM! DO WOULD YOU LIKE GREEN EGGS AND HAM? I DO NOT LIKE THEM,SAM-I-AM. I DO NOT LIKE GREEN EGGS AND HAM. This text takes up 192 bytes in a typical UTF-8 encoding. Let‚Äôs take a look at the LZSS encoded version. I AM SAM. &lt;10,10&gt;SAM I AM. THAT SAM-I-AM! T&lt;15,14&gt;I DO NOT LIKE&lt;29,15&gt; DO WOULD YOU LIKE GREEN EGGS AND HAM? I&lt;69,15&gt;EM,&lt;113,8&gt;.&lt;29,15&gt;GR&lt;64,16&gt;. This encoded, or compressed, version only takes 148 bytes to store (without a magic type to describe the file type), which is a 77% of the original file size, or a compression ratio of 1.3. Not bad! . Analysis . Now let‚Äôs take a second understand what‚Äôs happening before you start trying to conquer the world with LZSS. As we can see, the ‚Äútokens‚Äù are reducing the size of the file by referencing pieces of text that are longer than the actual token. Let‚Äôs look at the first line: . I AM SAM. &lt;10,10&gt;SAM I AM. The encoder works character by character. On the first character, ‚ÄòI‚Äô, it checks it‚Äôs search buffer to see if it‚Äôs already seen an ‚ÄòI‚Äô. The search buffer is essentially the encoder‚Äôs memory, for every character it encodes, it adds it into the search buffer so it can ‚Äúremember‚Äù it. Because it hasn‚Äôt seen an ‚ÄòI‚Äô already (the search buffer is empty), it just outputs an ‚ÄòI‚Äô, adds it to the search buffer, and moves to the next character. The next character is ‚Äò ‚Äò (a space). The encoder checks the search buffer to see if it‚Äôs seen a space before, and it hasn‚Äôt so it outputs the space and moves forward. Once it gets to the second space (after ‚ÄúI AM‚Äù), the LZ77 starts to come into play. It‚Äôs already seen a space before because it‚Äôs in the search buffer so it‚Äôs ready to output a token, but first it tries to maximize how much text the token is referencing. If it didn‚Äôt do this you could imagine that for every character it‚Äôs already seen it would output something similar to &lt;5,1&gt;, which is 5 times larger than any character. So once it finds a character that it‚Äôs already seen, it moves on to the next character and checks if it‚Äôs already seen the next character directly after the previous chracter. Once it finds a sequence of characters that it hasn‚Äôt already seen, then it goes back one character to the sequence of characters it‚Äôs already seen and prepares the token. Once the token is ready, the difference between LZ77 and LZSS starts to shine. At this point LZ77 simply outputs the token, adds the characters to the search buffer and continues. LZSS does something a little smarter, it will check to see if the size of the outputted token is larger than the text it‚Äôs representing. If so, it will output the text it represents, not the token, add the text to the search buffer, and continue. If not, it will output the token, add the text it represents to the search buffer and continue. Coming back to our example, the space character has already been seen, but a space followed by an ‚ÄúS‚Äù hasn‚Äôt been seen yet (‚Äú S‚Äù), so we prepare the token representing the space. The token in our case would be ‚Äú&lt;3,1&gt;‚Äù, which means go back three characters and copy 1 character(s). Next we check to see if our token is longer than our text, and ‚Äú&lt;3,1&gt;‚Äù is indeed longer than ‚Äú ‚Äú, so it wouldn‚Äôt make sense to output the token, so we output the space, add it to our search buffer, and continue. This entire process continues until we get to the ‚ÄúI AM SAM. ‚Äú. At this point we‚Äôve already seen an ‚ÄúI AM SAM. ‚Äú but haven‚Äôt seen an ‚ÄúI AM SAM. S‚Äù so we know our token will represent ‚ÄúI AM SAM. ‚Äú. Then we check to see if ‚ÄúI AM SAM. ‚Äú is longer than ‚Äú&lt;10,10&gt;‚Äù, which it is, so we output the token, add the text to our search buffer and go along. This process continues, encoding tokens and adding text to the search buffer character by character until it‚Äôs finished encoding everything. Takeaways . There‚Äôs a lot of information to unpack here, but the algorithm at a high level is quite simple: . | Loop character by character | Check if it‚Äôs seen the character before . | If so, check the next character and prepare a token to be outputted . | If the token is longer than the text it‚Äôs representing, don‚Äôt output a token | Add the text to the search buffer and continue | . | If not, add the character to the search buffer and continue | . | . It‚Äôs important to remember that no matter the outcome, token or no token, the text is always appended to the search buffer so it can always ‚Äúremember‚Äù the text it‚Äôs already seen. ",
    "url": "/algorithms/lzss/#implementing-an-encoder",
    "relUrl": "/algorithms/lzss/#implementing-an-encoder"
  },"22": {
    "doc": "LZSS",
    "title": "Implementation",
    "content": "Now let‚Äôs take a stab at building our very own version so we can understand it more deeply. As with most of these algorithms, we have an implementation written in Go in our raisin project. If you‚Äôre interested in what a more performant or real-world example of these algorithms looks like, be sure to check it out. However for this guide we‚Äôll use Python to make it more approachable so we can focus on understanding the algorithm and not the nuances of the language. Character Loop . Let‚Äôs get started with a simple loop that goes over each character for encoding. As we can see from our takeaways, the character-by-character loop is what powers LZSS. text = \"HELLO\" encoding = \"utf-8\" text_bytes = text.encode(encoding) for char in text_bytes: print(bytes([char]).decode(encoding)) # Print the character . Output: . H E L L O . Although the code is functionally pretty simple, there‚Äôs a few important things going on here. You can see that looping character-by-character isn‚Äôt as simple as for char in text, first we have to encode it and then loop over the encoding. This is because it converts our string into an array of bytes, represented as a Python object called bytes. When we print the character out, we have to convert it from a byte (represented as a Python int) back to a string so we can see it. The reason we do this is because a byte is really just a number from 0-255 as it is represented in your computer as 8 1‚Äôs and 0‚Äôs, called binary. If you don‚Äôt already have a basic understanding of how computers store our language, you should get acquainted with it on our getting started page. Search Buffers . Great, we have a basic program working which can loop over our text and print it out, but that‚Äôs pretty far off from compression so let‚Äôs keep going. The next step to our program is to implement our ‚Äúmemory‚Äù so the program can check to see if its already seen a character. text = \"HELLO\" encoding = \"utf-8\" text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes for char in text_bytes: print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer . We no longer need to output anything as we‚Äôre just adding each character to the search buffer with the append method. Checking Our Search buffer . Now let‚Äôs try to implement the LZ part of LZSS, we need to start looking backwards for characters we‚Äôve already seen. This can accomplished quite easily using the list.index method. for char in text_bytes: if char in search_buffer: print(f\"Found at {search_buffer.index(char)}\") print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer . Output: . H E L Found at 2 L O . Notice the if char in search_buffer, without this Python will throw an IndexError if the value is not in the list. Building Tokens . Now let‚Äôs build a token and output it when we find the character. i = 0 for char in text_bytes: if char in search_buffer: index = search_buffer.index(char) # The index where the character appears in our search buffer offset = i - index # Calculate the relative offset length = 1 # Set the length of the token (how many character it represents) print(f\"&lt;{offset},{length}&gt;\") # Build and print our token else: print(bytes([char]).decode(encoding)) # Print the character search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . H E L &lt;1,1&gt; O . We‚Äôre nearly there! This is actually a rough implementation of LZ77, however there‚Äôs one issue. If we have a word that repeats twice, it will copy each character instead of the entire word. text = \"SAM SAM\" . Output . S A M &lt;4,1&gt; &lt;4,1&gt; &lt;4,1&gt; . Note: &lt;4,1&gt; is technically correct as each character is represented 4 characters behind the beginning of the token. That‚Äôs not exactly right, we should see &lt;4,3&gt; instead of three &lt;4,1&gt; tokens. So let‚Äôs write some code that can check our search buffer for more than one character. Checking the Search Buffer for More Characters . Let‚Äôs modify our code to check the search buffer for more than one character. def elements_in_array(check_elements, elements): i = 0 offset = 0 for element in elements: if len(check_elements) &lt;= offset: # All of the elements in check_elements are in elements return i - len(check_elements) if check_elements[offset] == element: offset += 1 else: offset = 0 i += 1 return -1 text = \"SAM SAM\" encoding = \"utf-8\" text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes check_characters = [] # Array of integers, representing bytes i = 0 for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) print(f\"&lt;{offset},{length}&gt;\") # Build and print our token else: print(bytes([char]).decode(encoding)) # Print the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . S A M &lt;4,3&gt; . It works! But there‚Äôs quite a lot to unpack here so let‚Äôs go through it line by line. The first and largest addition is the elements_in_array function. The code here essentially checks to see if specific elements are within an array in an exact order. If so, it will return the index in the array where the elements start, and if not it will return -1. Moving on to our main function loop we can see now have check_characters defined at the top. This variable tracks what characters we‚Äôre looking for in our search_buffer. As we loop through, we use check_characters.append(char) to add the current character to the characters we‚Äôre searching. Then we check to see if check_characters can be found within search_buffer with elements_in_array. Now we have the best part: the logic. If we couldn‚Äôt find a match or it‚Äôs the last character we want to output something. If we couldn‚Äôt find more than one character in the search_buffer then that means check_characters minus the last character was found, so we‚Äôll output a token representing check_characters minus the last character. Otherwise, we couldn‚Äôt find a match for a single character so let‚Äôs just output that character. And that‚Äôs essentially LZ77! Try it out for yourself with some different strings to see for yourself. However you might notice that we‚Äôre trying to implement LZSS, not LZ77, so we have one more piece to implement. Comparing Token Sizes . This crucial piece is the process described earlier of comparing the size of tokens versus the text it represents. Essnetially we‚Äôre saying, if the token takes up more space than the text it‚Äôs representing then don‚Äôt output a token, just output the text. Lucky for us this is a pretty simple change. Our main loop now looks like so: . for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the character print(bytes(check_characters).decode(encoding)) # Print the characters else: print(token) # Print our token else: print(bytes([char]).decode(encoding)) # Print the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer i += 1 . Output: . S A M SAM . The key is the len(token) &gt; length which checks if the length of the token is longer than the length of the text it‚Äôs representing. If it is, it simply outputs the characters, otherwise it outputs the token. Sliding Windows . The last piece to the puzzle is something you might have noticed if you‚Äôre already trying to compress large file: the search buffer gets big. Let‚Äôs say we‚Äôre compressing a 1 Gb file. After we go over each character, we add it to the search buffer and continue, though each iteration we also search the entire search buffer for certain characters. This quickly adds up for larger files. In our 1 Gb file scenario, near the end we‚Äôll have to search almost 1 billion bytes to encode a single character. It should be pretty obvious that this very inefficient. And unfortunately, there is no magic solution, you have to make a tradeoff. With every compression algorithm you have to decide between speed and compression ratio. Do you want a fast algorithm that can‚Äôt reduce the file size very much, or a slow algorithm that reduces the file size more? The answer is: it depends. And so, the tradeoff in LZ77‚Äôs case is to create a ‚Äúsliding window‚Äù. The ‚Äúsliding window‚Äù is actually quite simple, all you do is cap off the maximum size of the search buffer. When you add a character to the search buffer that makes it larger than the maximum size of the sliding window then you remove the first character. That way the window is ‚Äúsliding‚Äù as you move through the file, and the algorithm doesn‚Äôt slow down! . max_sliding_window_size = 4096 ... for char in text_bytes: ... search_buffer.append(char) # Add the character to our search buffer if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer ... These changes should be pretty self-explanatory. We‚Äôre just checking to see if the length of the search_buffer is greater than the max_sliding_window_size, and if so we pop the first element off of the search_buffer. Keep in mind that while a maximum sliding window size of 4096 character is typical, it may be hard to use during testing, try setting it much lower (like 3-4) and test it with some different strings to see how it works. Putting it all together . That‚Äôs everything that makes up LZSS, but for the sake of completing our example, let‚Äôs clean it up so we can call a function with some text, an optional max_sliding_window_size, and have it return the encoded text, rather than just printing it out. encoding = \"utf-8\" def encode(text, max_sliding_window_size=4096): text_bytes = text.encode(encoding) search_buffer = [] # Array of integers, representing bytes check_characters = [] # Array of integers, representing bytes output = [] # Output array i = 0 for char in text_bytes: check_characters.append(char) index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if index == -1 or i == len(text_bytes) - 1: if len(check_characters) &gt; 1: index = elements_in_array(check_characters[:-1], search_buffer) offset = i - index - len(check_characters) + 1 # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the character output.extend(check_characters) # Output the characters else: output.extend(token.encode(encoding)) # Output our token else: output.extend(check_characters) # Output the character check_characters = [] search_buffer.append(char) # Add the character to our search buffer if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer i += 1 return bytes(output) print(encode(\"SAM SAM\", 1).decode(encoding)) print(encode(\"supercalifragilisticexpialidocious supercalifragilisticexpialidocious\", 1024).decode(encoding)) print(encode(\"LZSS will take over the world!\", 256).decode(encoding)) print(encode(\"It even works with üòÄs thanks to UTF-8\", 16).decode(encoding)) . The function definition is pretty simple, we can just move our text and max_sliding_window_size outside of the function and wrap our code in a function definition. Then we simply call it with some different values to test it, and that‚Äôs it! . The finished code can be found in lzss.py in the examples GitHub repo. Lastly, there‚Äôs a few bug in our program that we encounter with larger files. If we have some text, for example: . ISAM YAM SAM . When the encoder gets to the space right before the ‚ÄúSAM‚Äù, it will look for a space in the search buffer which it finds. Then it will search for a space and an ‚ÄúS‚Äù (‚Äú S‚Äù) which it doesn‚Äôt find, so it continues and starts looking for an ‚ÄúA‚Äù. The issue here is that it skips looking for an ‚ÄúS‚Äù and continues to encode the ‚ÄúAM‚Äù not the ‚ÄúSAM‚Äù. In some rare circumstances the code may generate a reference that with a length that is larger than its offset which will result in an error. To fix this, we‚Äôll need to rewrite the logic in our encoder a little bit. for char in text_bytes: index = elements_in_array(check_characters, search_buffer) # The index where the characters appears in our search buffer if elements_in_array(check_characters + [char], search_buffer) == -1 or i == len(text_bytes) - 1: if i == len(text_bytes) - 1 and elements_in_array(check_characters + [char], search_buffer) != -1: # Only if it's the last character then add the next character to the text the token is representing check_characters.append(char) if len(check_characters) &gt; 1: index = elements_in_array(check_characters, search_buffer) offset = i - index - len(check_characters) # Calculate the relative offset length = len(check_characters) # Set the length of the token (how many character it represents) token = f\"&lt;{offset},{length}&gt;\" # Build our token if len(token) &gt; length: # Length of token is greater than the length it represents, so output the characters output.extend(check_characters) # Output the characters else: output.extend(token.encode(encoding)) # Output our token search_buffer.extend(check_characters) # Add the characters to our search buffer else: output.extend(check_characters) # Output the character search_buffer.extend(check_characters) # Add the characters to our search buffer check_characters = [] check_characters.append(char) if len(search_buffer) &gt; max_sliding_window_size: # Check to see if it exceeds the max_sliding_window_size search_buffer = search_buffer[1:] # Remove the first element from the search_buffer i += 1 . To fix the first issue we add the current char to check_characters only at the end and check to see if check_characters + [char] is found. If not we know that check_characters is found so we can continue as normal, and check_characters gets cleared before char is added onto check_characters for the next iteration. We also implement a check on the last iteration to add the current char to check_characters as otherwise our logic wouldn‚Äôt be run on the last character and a token wouldn‚Äôt be created. To resolve the other problem we simply have to move the search_buffer.append(char) calls up into our logic and change them to search_buffer.extend(check_characters). This way we only update our search buffer when we‚Äôve already tried to find a token. ",
    "url": "/algorithms/lzss/#implementation",
    "relUrl": "/algorithms/lzss/#implementation"
  },"23": {
    "doc": "LZSS",
    "title": "Implementing a Decoder",
    "content": "What‚Äôs the use of encoding something some text if we can‚Äôt decode it? For that we‚Äôll need to build ourselves a decoder. Luckily for us, building a decoder is actually much simpler than an encoder because all it needs to know how to do is convert a token (‚Äú&lt;5,2&gt;‚Äù) into the literal text it represents. The decoder doesn‚Äôt care about search buffers, sliding windows, or token lengths, it only has one job. So, let‚Äôs get started. We‚Äôre going to decode character-by-character just like our encoder so we‚Äôll start with our main loop inside of a function. We‚Äôll also need to encode and decode the strings so we‚Äôll keep the encoding = \"utf-8\". encoding = \"utf-8\" def decode(text): text_bytes = text.encode(encoding) # The text encoded as bytes output = [] # The output characters for char in text_bytes: output.append(char) # Add the character to our output return bytes(output) print(decode(\"supercalifragilisticexpialidocious &lt;35,34&gt;\").decode(encoding)) . Here we‚Äôre setting up the structure for the rest of our decoder by setting up our main loop and declaring everything within a neat self-contained function. Identifying Tokens . The next step is to start doing some actual decoding. The goal of our decoder is to convert a token into text, so we need to first identify a token and extract our offset and length before we can convert it into text. Notice the various components of a token that need to be identified and extracted so we can find the text they represent . Let‚Äôs make a small change so we can identify the start and end of a token. for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: print(\"Found opening of a token\") elif char == \"&gt;\".encode(encoding)[0]: print(\"Found closing of a token\") output.append(char) # Add the character to our output return bytes(output) . Because we‚Äôre going character-by-character we can simply check to see if the character is a token opening character or closing character to tell if we‚Äôre inside a token. Let‚Äôs add some more code to track the numbers between the comma, our seperator. inside_token = False scanning_offset = True length = [] # Length number encoded as bytes offset = [] # Offset number encoded as bytes for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: inside_token = True # We're now inside a token scanning_offset = True # We're now looking for the length number elif char == \",\".encode(encoding)[0]: scanning_offset = False elif char == \"&gt;\".encode(encoding)[0] and inside_token: inside_token = False # We're no longer inside a token # Convert length and offsets to an integer length_num = int(bytes(length).decode(encoding)) offset_num = int(bytes(offset).decode(encoding)) print(f\"Found token with length: {length_num}, offset: {offset_num}\") # Reset length and offset length, offset = [], [] elif inside_token: if scanning_offset: offset.append(char) else: length.append(char) output.append(char) # Add the character to our output . Output: . Found token with length: 34, offset: 35 supercalifragilisticexpialidocious &lt;35,34&gt; . We now have a bunch of if statements that give our loop some more control flow. Let‚Äôs go over the changes. First off we have four new variables outside of the loop: . | inside_token - Tracks whether or not we‚Äôre inside a token | scanning_offset - Tracks whether we‚Äôre currently scanning for the offset number or the length number (1st or 2nd number in the token) | length - Used to store the bytes (or characters) that represent the token‚Äôs length | offset- Used to store the bytes (or characters) that represent the token‚Äôs offset | . Inside of the loop, we check if the character is a &lt;, ,, or a &gt; and modify the variables accordingly to track where we are. If the character isn‚Äôt any of those and we‚Äôre inside a token then we want to add the character to either the offset or length because that means the character is an offset or length. Lastly, if the character is a &gt;, that means we‚Äôre exiting the token, so let‚Äôs convert our length and offset into a Python int. We have to do this because they‚Äôre currently represented as a list of bytes, so we need to convert those bytes into a Python string and convert that string into an int. Then we finally print that we‚Äôve found a token. Translating Tokens . Now we have one last step left: translating tokens into the text they represent. Thanks to Python list slicing this is quite simple. for char in text_bytes: if char == \"&lt;\".encode(encoding)[0]: inside_token = True # We're now inside a token scanning_offset = True # We're now looking for the length number token_start = i elif char == \",\".encode(encoding)[0]: scanning_offset = False elif char == \"&gt;\".encode(encoding)[0] and inside_token: inside_token = False # We're no longer inside a token # Convert length and offsets to an integer length_num = int(bytes(length).decode(encoding)) offset_num = int(bytes(offset).decode(encoding)) # Get text that the token represents referenced_text = output[-offset_num:][:length_num] output.extend(referenced_text) # referenced_text is a list of bytes so we use extend to add each one to output # Reset length and offset length, offset = [], [] elif inside_token: if scanning_offset: offset.append(char) else: length.append(char) else: output.append(char) # Add the character to our output return bytes(output) . Output: . supercalifragilisticexpialidocious supercalifragilisticexpialidocious . In order to calculate the piece of text that a token is referencing we can simply use our offset and length to find the text from the current output. We use a negative slice to grab all the characters backwards from offset_num and grab up to length_num elements. This results in a referenced_text that represents the token references. Finally we add the referenced_text to our output and we‚Äôre finished. Lastly, we‚Äôll only want to add a character to the output if we‚Äôre not in a token so we add an else to the end of our logic which only runs if we‚Äôre not in a token. And that‚Äôs it! We now have a LZSS decoder, and by extension, an LZ77 decoder as decoders don‚Äôt need to worry about outputting a token only if it‚Äôs greater than the referenced text. ",
    "url": "/algorithms/lzss/#implementing-a-decoder",
    "relUrl": "/algorithms/lzss/#implementing-a-decoder"
  },"24": {
    "doc": "LZSS",
    "title": "Implementation Conclusion",
    "content": "We‚Äôve gone through step-by-step building an encoder and decoder and learned the purpose of each component. Let‚Äôs do some basic benchmarks to see how well it works. if __name__ == \"__main__\": encoded = encode(text).decode(encoding) decoded = decode(encoded).decode(encoding) print(f\"Original: {len(text)}, Encoded: {len(encoded)}, Decoded: {len(decoded)}\") print(f\"Lossless: {text == decoded}\") print(f\"Compressed size: {(len(encoded)/len(text)) * 100}%\") . Using the text as Green Eggs and Ham by Doctor Seuss, we see the output: . Original: 3463 bytes, Encoded: 1912 bytes, Decoded: 3463 bytes Lossless: True Compressed size: 55.21224371931851% . LZSS just reduced the file size by 45%, not bad! . One thing to keep in mind is that when we refer to a ‚Äúcharacter‚Äù, we really mean a ‚Äúbyte‚Äù. Our loop runs byte-by-byte, not character-by-character. This distinction is minor but significant. In the world of encodings, not every character is a single byte. For example in utf-8, any english letter or symbol is a single byte, but more complicated characters like arabic, mandarin, or emoji characters require multiple bytes despite being a single ‚Äúcharacter‚Äù. | 4 bytes - üòÄ | 1 byte - H | 3 bytes - ËØù | 6 bytes - ŸäŸéŸë | . If you‚Äôre interested in learning more about how bytes work, check out the Wikipedia articles on Bytes and Unicode. ",
    "url": "/algorithms/lzss/#implementation-conclusion",
    "relUrl": "/algorithms/lzss/#implementation-conclusion"
  },"25": {
    "doc": "LZSS",
    "title": "LZSS",
    "content": " ",
    "url": "/algorithms/lzss/",
    "relUrl": "/algorithms/lzss/"
  },"26": {
    "doc": "Unix Magic Types",
    "title": "Unix Magic Types",
    "content": "Stub. ",
    "url": "/reference/magic_types/",
    "relUrl": "/reference/magic_types/"
  },"27": {
    "doc": "Overview of Algorithms",
    "title": "Overview of Algorithms",
    "content": "The following is intended to be a comprehensive list of lossless compression algorithms (in no particular order), however if you feel like an algorithm is missing, please let us know. | Run-length Coding | Range Coding | Lempel-Ziv . | LZ77 | LZ78 | LZSS | LZW | . | Variable-length Coding | Huffman Coding | Arithmetic Coding | Dynamic Markov Compression | FLATE | . For a more complete list, check out these Wikipedia pages on lossless algorithms and lossy algorithms. ",
    "url": "/algorithms/overview/",
    "relUrl": "/algorithms/overview/"
  },"28": {
    "doc": "Reference",
    "title": "Reference",
    "content": "This section serves to be a reference to topics that aren‚Äôt directly related to compression, but inveitably come into play. A basic understanding of these concepts are invaluable when building your own implementation or algorithm. ",
    "url": "/reference/reference/",
    "relUrl": "/reference/reference/"
  }
}
